How to Reshape Input Data for Long Short-Term Memory Networks in Keras
https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/
2017-08-29
It can be difficult to understand how to prepare your sequence data for input to an LSTM model.
Often there is confusion around how to define the input layer for the LSTM model.
There is also confusion about how to convert your sequence data that may be a 1D or 2D matrix of numbers to the required 3D format of the LSTM input layer.
In this tutorial, you will discover how to define the input layer to LSTM models and how to reshape your loaded input data for LSTM models.
After completing this tutorial, you will know:
- How to define an LSTM input layer.
- How to reshape a one-dimensional sequence data for an LSTM model and define the input layer.
- How to reshape multiple parallel series data for an LSTM model and define the input layer.
Kick-start your project with my new book Long Short-Term Memory Networks With Python, including step-by-step tutorials and the Python source code files for all examples.
Let’s get started.
How to Reshape Input for Long Short-Term Memory Networks in KerasPhoto by Global Landscapes Forum, some rights reserved.
# Tutorial Overview
This tutorial is divided into 4 parts; they are:
- LSTM Input Layer
- Example of LSTM with Single Input Sample
- Example of LSTM with Multiple Input Features
- Tips for LSTM Input
The LSTM input layer is specified by the “input_shape” argument on the first hidden layer of the network.
This can make things confusing for beginners.
For example, below is an example of a network with one hidden LSTM layer and one Dense output layer.
In this example, the LSTM() layer must specify the shape of the input.
The input to every LSTM layer must be three-dimensional.
The three dimensions of this input are:
- Samples. One sequence is one sample. A batch is comprised of one or more samples.
- Time Steps. One time step is one point of observation in the sample.
- Features. One feature is one observation at a time step.
This means that the input layer expects a 3D array of data when fitting the model and when making predictions, even if specific dimensions of the array contain a single value, e.g. one sample or one feature.
When defining the input layer of your LSTM network, the network assumes you have 1 or more samples and requires that you specify the number of time steps and the number of features. You can do this by specifying a tuple to the “input_shape” argument.
For example, the model below defines an input layer that expects 1 or more samples, 50 time steps, and 2 features.
Now that we know how to define an LSTM input layer and the expectations of 3D inputs, let’s look at some examples of how we can prepare our data for the LSTM.
# Example of LSTM With Single Input Sample
Consider the case where you have one sequence of multiple time steps and one feature.
For example, this could be a sequence of 10 values:
We can define this sequence of numbers as a NumPy array.
We can then use the reshape() function on the NumPy array to reshape this one-dimensional array into a three-dimensional array with 1 sample, 10 time steps, and 1 feature at each time step.
The reshape() function when called on an array takes one argument which is a tuple defining the new shape of the array. We cannot pass in any tuple of numbers; the reshape must evenly reorganize the data in the array.
Once reshaped, we can print the new shape of the array.
Putting all of this together, the complete example is listed below.
Running the example prints the new 3D shape of the single sample.
This data is now ready to be used as input (X) to the LSTM with an input_shape of (10, 1).

# Example of LSTM with Multiple Input Features
Consider the case where you have multiple parallel series as input for your model.
For example, this could be two parallel series of 10 values:
We can define these data as a matrix of 2 columns with 10 rows:
This data can be framed as 1 sample with 10 time steps and 2 features.
It can be reshaped as a 3D array as follows:
Putting all of this together, the complete example is listed below.
Running the example prints the new 3D shape of the single sample.
This data is now ready to be used as input (X) to the LSTM with an input_shape of (10, 2).

# Longer Worked Example
For a complete end-to-end worked example of preparing data, see this post:
- How to Prepare Univariate Time Series Data for Long Short-Term Memory Networks
# Tips for LSTM Input
This section lists some tips to help you when preparing your input data for LSTMs.
- The LSTM input layer must be 3D.
- The meaning of the 3 input dimensions are: samples, time steps, and features.
- The LSTM input layer is defined by the input_shape argument on the first hidden layer.
- The input_shape argument takes a tuple of two values that define the number of time steps and features.
- The number of samples is assumed to be 1 or more.
- The reshape() function on NumPy arrays can be used to reshape your 1D or 2D data to be 3D.
- The reshape() function takes a tuple as an argument that defines the new shape.
# Further Reading
This section provides more resources on the topic if you are looking go deeper.
- Recurrent Layers Keras API
- Numpy reshape() function API
- How to Convert a Time Series to a Supervised Learning Problem in Python
- Time Series Forecasting as Supervised Learning
# Summary
In this tutorial, you discovered how to define the input layer for LSTMs and how to reshape your sequence data for input to LSTMs.
Specifically, you learned:
- How to define an LSTM input layer.
- How to reshape a one-dimensional sequence data for an LSTM model and define the input layer.
- How to reshape multiple parallel series data for an LSTM model and define the input layer.
Do you have any questions?
Ask your questions in the comments below and I will do my best to answer.
# Develop LSTMs for Sequence Prediction Today!
...with just a few lines of python code
Discover how in my new Ebook:
Long Short-Term Memory Networks with Python
It provides self-study tutorials on topics like:
CNN LSTMs, Encoder-Decoder LSTMs, generative models, data preparation, making predictions and much more...
Skip the Academics. Just Results.
See What's Inside

'''
model = Sequential()
model.add(LSTM(32))
model.add(Dense(1))
model = Sequential()
model.add(LSTM(32, input_shape=(50, 2)))
model.add(Dense(1))
0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0
from numpy import array
data = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])
data = data.reshape((1, 10, 1))
print(data.shape)
from numpy import array
data = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])
data = data.reshape((1, 10, 1))
print(data.shape)
(1, 10, 1)
model = Sequential()
model.add(LSTM(32, input_shape=(10, 1)))
model.add(Dense(1))
series 1: 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0
series 2: 1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1
from numpy import array
data = array([
	[0.1, 1.0],
	[0.2, 0.9],
	[0.3, 0.8],
	[0.4, 0.7],
	[0.5, 0.6],
	[0.6, 0.5],
	[0.7, 0.4],
	[0.8, 0.3],
	[0.9, 0.2],
	[1.0, 0.1]])
data = data.reshape(1, 10, 2)
from numpy import array
data = array([
	[0.1, 1.0],
	[0.2, 0.9],
	[0.3, 0.8],
	[0.4, 0.7],
	[0.5, 0.6],
	[0.6, 0.5],
	[0.7, 0.4],
	[0.8, 0.3],
	[0.9, 0.2],
	[1.0, 0.1]])
data = data.reshape(1, 10, 2)
print(data.shape)
(1, 10, 2)
model = Sequential()
model.add(LSTM(32, input_shape=(10, 2)))
model.add(Dense(1))
'''
