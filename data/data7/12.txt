How to Save Gradient Boosting Models with XGBoost in Python
https://machinelearningmastery.com/save-gradient-boosting-models-xgboost-python/
2016-08-23
XGBoost can be used to create some of the most performant models for tabular data using the gradient boosting algorithm.
Once trained, it is often a good practice to save your model to file for later use in making predictions new test and validation datasets and entirely new data.
In this post you will discover how to save your XGBoost models to file using the standard Python pickle API.
After completing this tutorial, you will know:
- How to save and later load your trained XGBoost model using pickle.
- How to save and later load your trained XGBoost model using joblib.
Kick-start your project with my new book XGBoost With Python, including step-by-step tutorials and the Python source code files for all examples.
Let’s get started.
- Update Jan/2017: Updated to reflect changes in scikit-learn API version 0.18.1.
- Update Mar/2018: Added alternate link to download the dataset as the original appears to have been taken down.
- Update Oct/2019: Updated to use Joblib API directly.
How to Save Gradient Boosting Models with XGBoost in PythonPhoto by Keoni Cabral, some rights reserved.




# Serialize Your XGBoost Model with Pickle
Pickle is the standard way of serializing objects in Python.
You can use the Python pickle API to serialize your machine learning algorithms and save the serialized format to a file, for example:
Later you can load this file to deserialize your model and use it to make new predictions, for example:
The example below demonstrates how you can train a XGBoost model on the Pima Indians onset of diabetes dataset, save the model to file and later load it to make predictions.
Download the dataset and save it to your current working directory.
- Dataset File.
- Dataset Details.
The full code listing is provided below for completeness.
Running this example saves your trained XGBoost model to the pima.pickle.dat pickle file in the current working directory.

Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.

After loading the model and making predictions on the training dataset, the accuracy of the model is printed.

# Serialize XGBoost Model with joblib
Joblib is part of the SciPy ecosystem and provides utilities for pipelining Python jobs.
The Joblib API provides utilities for saving and loading Python objects that make use of NumPy data structures, efficiently. It may be a faster approach for you to use with very large models.
The API looks a lot like the pickle API, for example, you may save your trained model as follows:
You can later load the model from file and use it to make predictions as follows:
The example below demonstrates how you can train an XGBoost model for classification on the Pima Indians onset of diabetes dataset, save the model to file using Joblib and load it at a later time in order to make predictions.
Running the example saves the model to file as pima.joblib.dat in the current working directory and also creates one file for each NumPy array within the model (in this case two additional files).

Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.

After the model is loaded, it is evaluated on the training dataset and the accuracy of the predictions is printed.

# Summary
In this post, you discovered how to serialize your trained XGBoost models and later load them in order to make predictions.
Specifically, you learned:
- How to serialize and later load your trained XGBoost model using the pickle API.
- How to serialize and later load your trained XGBoost model using the joblib API.
Do you have any questions about serializing your XGBoost models or about this post? Ask your questions in the comments and I will do my best to answer.
# Discover The Algorithm Winning Competitions!
...with just a few lines of Python
Discover how in my new Ebook:
XGBoost With Python
It covers self-study tutorials like:
Algorithm Fundamentals, Scaling, Hyperparameters, and much more...
Skip the Academics. Just Results.
See What's Inside

'''
# save model to file
pickle.dump(model, open("pima.pickle.dat", "wb"))
# load model from file
loaded_model = pickle.load(open("pima.pickle.dat", "rb"))
# Train XGBoost model, save to file using pickle, load and make predictions
from numpy import loadtxt
import xgboost
import pickle
from sklearn import model_selection
from sklearn.metrics import accuracy_score
# load data
dataset = loadtxt('pima-indians-diabetes.csv', delimiter=",")
# split data into X and y
X = dataset[:,0:8]
Y = dataset[:,8]
# split data into train and test sets
seed = 7
test_size = 0.33
X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, Y, test_size=test_size, random_state=seed)
# fit model no training data
model = xgboost.XGBClassifier()
model.fit(X_train, y_train)
# save model to file
pickle.dump(model, open("pima.pickle.dat", "wb"))

# some time later...

# load model from file
loaded_model = pickle.load(open("pima.pickle.dat", "rb"))
# make predictions for test data
y_pred = loaded_model.predict(X_test)
predictions = [round(value) for value in y_pred]
# evaluate predictions
accuracy = accuracy_score(y_test, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))
pima.pickle.dat
Accuracy: 77.95%
# save model to file
joblib.dump(model, "pima.joblib.dat")
# load model from file
loaded_model = joblib.load("pima.joblib.dat")
# Train XGBoost model, save to file using joblib, load and make predictions
from numpy import loadtxt
from xgboost import XGBClassifier
from joblib import dump
from joblib import load
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
# load data
dataset = loadtxt('pima-indians-diabetes.csv', delimiter=",")
# split data into X and y
X = dataset[:,0:8]
Y = dataset[:,8]
# split data into train and test sets
seed = 7
test_size = 0.33
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)
# fit model on training data
model = XGBClassifier()
model.fit(X_train, y_train)
# save model to file
dump(model, "pima.joblib.dat")
print("Saved model to: pima.joblib.dat")

# some time later...

# load model from file
loaded_model = load("pima.joblib.dat")
print("Loaded model from: pima.joblib.dat")
# make predictions for test data
predictions = loaded_model.predict(X_test)
# evaluate predictions
accuracy = accuracy_score(y_test, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))
pima.joblib.dat
pima.joblib.dat_01.npy
pima.joblib.dat_02.npy
Accuracy: 77.95%
'''
